---
title: 5. 內插與曲線擬合
description: 碩士課程\應用數值分析
draft: False
disableSPA: True
discussion: True
date: 2024-11-13
tags:
  - 碩士課程
  - 應用數值分析
---
# 課程筆記
## Interpolation (內插)
### 線性內插 (Linear Interpolation)
- 優缺點：
  - 優點：可以求解出多項式的係數，可以重複使用。
  - 缺點：需要解聯立方程式，計算較為複雜。
> 由於此較為簡單，直接參考以下例題求解即可。

### 拉格朗日多項式內插 (Lagrange Polynomials)
- 優缺點：
  - 優點：
    - 可直接計算出多項式的值，不需要先求出未知數。
    - 結構簡單，不需解連立方成組，易於程式化計算。
  - 缺點：
    - 當需重複計算時，每次都需要重新計算。
- 相對高階多項式內插，拉格朗日多像式內插在數值計算上更為方便：
  - 結構更為簡單，易於程式化計算。
  - 可以直接計算出多項式的值，不需要先求出未知數。
- 方法：
  1. Lagrange Polynomials: $P_n(x) = \sum_{i=0}^{n}L_i(x)f(x_i) = \sum_{i=0}^{n}\left[\prod_{j=0,j\neq i}^{n} \cfrac{(x-x_j)}{(x_i-x_j)} \right]f(x_i)$
  2. 舉例當 $n=3$：$P_3(x)$ = $\cfrac{(x-x_1)(x-x_2)(x-x_3)}{(x_0-x_1)(x_0-x_2)(x_0-x_3)}$ $f(x_0)$ + $\cfrac{(x-x_0)(x-x_2)(x-x_3)}{(x_1-x_0)(x_1-x_2)(x_1-x_3)}$ $f(x_1)$ + $\cfrac{(x-x_0)(x-x_1)(x-x_3)}{(x_2-x_0)(x_2-x_1)(x_2-x_3)}$ $f(x_2)$ + $\cfrac{(x-x_0)(x-x_1)(x-x_2)}{(x_3-x_0)(x_3-x_1)(x_3-x_2)}$ $f(x_3)$ 

##### 除差法 (Divided Difference) 
- 優點：
  - 計算簡單，不需要解聯立方程式。
  - 可以直接計算出多項式的係數（可重複使用）。
  - 可以快速找到多項式的誤差項。
- 方法：n 階除差多項式：$P_n(x)=a_0$+$a_1(x-x_0)$+$a_2$ $(x-x_0)$ $(x-x_1)$+$...$+$a_n$ $(x-x_0)$ $(x-x_1)$ $...$ $(x-x_{n-1})$
  - 因此：
    - 0 階除差（1 個數據可以找出 1 個 0 階除差）：$f(x_s)=f_s=f_s^{[0]}$
    - 1 階除差（2 個數據可以找出 1 個 1 階除差）：
      - $f[x_0,x_1] = \cfrac{f(x_1) - f(x_0)}{x_1 - x_0}=f_0^{[1]}$
      - $f[x_1,x_2] = \cfrac{f(x_2) - f(x_1)}{x_2 - x_1}=f_1^{[1]}$
      - 依此類推
    - 2 階除差（3 個數據可以找出 1 個 2 階除差）：
      - $f[x_0,x_1,x_2] = \cfrac{f[x_1,x_2] - f[x_0,x_1]}{x_2 - x_0} = \cfrac{f_1^{[1]} - f_0^{[1]}}{x_2 - x_0} = f_0^{[2]}$
      - $f[x_1,x_2,x_3] = \cfrac{f[x_2,x_3] - f[x_1,x_2]}{x_3 - x_1} = \cfrac{f_2^{[1]} - f_1^{[1]}}{x_3 - x_1} = f_1^{[2]}$
      - 依此類推
    - n 階除差（n+1 個數據可以找出 1 個 n 階除差）：
      - $f[x_0,x_1,...,x_n] = \cfrac{f[x_1,x_2,...,x_n] - f[x_0,x_1,...,x_{n-1}]}{x_n - x_0} = \cfrac{f_1^{[n-1]} - f_0^{[n-1]}}{x_n - x_0} = f_0^{[n]}$
      - 依此類推
- 方法：除差表 (Divided Difference Table)：
  - | $x_i$ | $f(x_i)$ | $f_i^{[1]}$ | $f_i^{[2]}$ | $f_i^{[3]}$ |
      |:---:|:-----:|:----:|:-----:|:------:|
      | $x_0$ | $f(x_0)$ | $f[x_0, x_1]=\cfrac{f(x_1)-f(x_0)}{x_1-x_0}$ | $f[x_0, x_1, x_2]=\cfrac{f[x_1, x_2]-f[x_0, x_1]}{x_2-x_0}$ | $f[x_0, x_1, x_2, x_3]=\cfrac{f[x_1, x_2, x_3]-f[x_0, x_1, x_2]}{x_3-x_0}$ |
      | $x_1$ | $f(x_1)$ | $f[x_1, x_2]=\cfrac{f(x_2)-f(x_1)}{x_2-x_1}$ | $f[x_1, x_2, x_3]=\cfrac{f[x_2, x_3]-f[x_1, x_2]}{x_3-x_1}$ | |
      | $x_2$ | $f(x_2)$ | $f[x_2, x_3]=\cfrac{f(x_2)-f(x_3)}{x_2-x_3}$ | | |
      | $x_3$ | $f(x_3)$ | | | |
  - 由於：
      - $x=x_0\rightarrow P_n(x_0)=a_0=f_0$
      - $x=x_1\rightarrow P_n(x_1)=a_0+a_1(x_1-x_0)=f_1\rightarrow a_1=\cfrac{f_1-f_0}{x_1-x_0}=f_0^{[1]}$
      - $x=x_2\rightarrow P_n(x_2)=a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)\rightarrow a_2=\cfrac{f_1^{[1]}-f_0^{[1]}}{x_2-x_0}=f_0^{[2]}$
      - 依此類推，故：$x=x_n\rightarrow a_n=f_0^{[n]}$，也就是除差表中第一列的第 n 個數值。

### 舉例
> 假設已知：
> | x | f(x) |
> |---|------|
> |3.2| 22.0 |
> |2.7| 17.8 |
> |1.0| 14.2 |
> |4.8| 38.3 |
> |5.6| 51.7 |
> 
> Find f(3.0)

#### Solution-線性內插
1. $\cfrac{f(3.0)-f_1}{f_0-f_1} = \cfrac{3.0-x_1}{x_0-x_1}$
2. $\cfrac{f(3.0)-17.8}{22.0-17.8} = \cfrac{3.0-2.7}{3.2-2.7}$
3. 得 $f(3.0) = \underline{20.32　\blacksquare}$

#### Solution-高階多項式內插

1. 假設三階多項式：$f(x) = a_0 + a_1x + a_2x^2 + a_3x^3$
2. 故得方程組 (四個未知數，需要四筆數據求未知數)：
   - $(3.2)=a_0 + 3.2a_1 + 3.2^2a_2 + 3.2^3a_3 = 22.0$
   - $(2.7)=a_0 + 2.7a_1 + 2.7^2a_2 + 2.7^3a_3 = 17.8$
   - $(1.0)=a_0 + 1.0a_1 + 1.0^2a_2 + 1.0^3a_3 = 14.2$
   - $(4.8)=a_0 + 4.8a_1 + 4.8^2a_2 + 4.8^3a_3 = 38.3$
3. 使用高斯消去法 || 高斯-賽得法求解未知數，得：
   - $a_0 = 24.3499$
   - $a_1 = -16.1177$
   - $a_2 = 6.1953$
   - $a_3 = 0.5275$
4. 故 $f(3.0) = 24.3499 - 16.1177(3.0) + 6.1953(3.0)^2 + 0.5275(3.0)^3 = \underline{20.21　\blacksquare}$

#### Solution-除差法
1. 使用除差表計算：
    - | $x_i$ | $f(x_i)=f_i^{[0]}$ | $f_i^{[1]}$ | $f_i^{[2]}$ | $f_i^{[3]}$ | $f_i^{[4]}$ |
      |---|:------:|------:|------|------|-----|
      | 3.2 | <span style="color:red;">22.0</span> | <span style="color:red;">8.400</span>  | <span style="color:red;">2.856</span> | <span style="color:red;">-0.528</span> | <span style="color:red;">0.256</span> |
      | 2.7 | 17.8 | 2.118  | 2.012 | 0.0865 |
      | 1.0 | 14.2 | 6.342  | 2.263 | 
      | 4.8 | 38.3 | 16.750 | | |
      | 5.6 | 51.7 | | | |
2. 由於 $x=x_n\rightarrow a_n=f_0^{[n]}$。故，$a_1$ ~ $a_n$ 即為上表中紅色部分。
3. 故 <div style="text-align: initial">$P_3(x)$=$22.0$+$8.400(x-3.2)$+$2.856(x-3.2)(x-2.7)$-$0.528(x-3.2)(x-2.7(x-1.0)$</div>
4. 因此，$f(3.0)=P_3(3.0)=\underline{20.21　\blacksquare}$
5. 甚至當要使用 n = 4 時：<div style="text-align: initial">$P_4(x)=22.0$+$8.400(x-3.2)$+$2.856(x-3.2)(x-2.7)$-$0.528(x-3.2)(x-2.7)(x-1.0)$+$0.256(x-3.2)(x-2.7)(x-1.0)(x-4.8)$=$P_3(x)$+$0.256(x-3.2(x-2.7)(x-1.0)(x-4.8)$</div>
   - 如果 $P_4(x)$ 更為精準（數列必須為強收斂），則 $0.256(x-3.2)(x-2.7)(x-1.0)(x-4.8)$ 為 $P_3(x)$ 與 $P_4(x)$ 的誤差項。 
     - 如果不是強收斂，則此筆數據不適合使用此種多項式內插。
       - 有時候，在高階方程式不一定能夠得到更好的結果，因此需要觀察數據的特性。

### 等間距內插 (Evenly Spaced Interpolation)
- 使用情境：當數據點間距相等時（如網格數據的內插），可以使用等間距內插法。
- 原理：
  - 等間距，表 $\Delta x = h \in \text{constant}$
  - 以差分方式取代除差
    - $\Delta f_i = f_{i+1} - f_i$
    - $\Delta^2 f_i = \Delta f_{i+1} - \Delta f_i=(f_{i+2}-f_{i+1})-(f_{i+1}-f_i)=f_{i+2}-2f_{i+1}+f_i$
    - $\Delta^3 f_i = \Delta^2 f_{i+1} - \Delta^2 f_i=(f_{i+3}-2f_{i+2}+f_{i+1})-(f_{i+2}-2f_{i+1}+f_i)=f_{i+3}-3f_{i+2}+3f_{i+1}-f_i$
    - 依此類推：$\Delta^n f_i = f_{i+n}-nf_{i+n-1}+\cfrac{n(n-1)}{2!}f_{i+n-2}-...$

#### N-G 前推差分多項式 (Newton-Gregory Forward Polynomial)
- $P_n(x) = f_0 + S\Delta f_0 + \cfrac{S(S-1)}{2!}\Delta^2 f_0 + \cfrac{S(S-1)(S-2)}{3!}\Delta^3 f_0 + ... + \cfrac{S(S-1)...(S-n+1)}{n!}\Delta^n f_0$
  - $=f_0 + \left( \begin{array}{c} S \\ 1 \end{array} \right) \Delta f_0 + \left( \begin{array}{c} S \\ 2 \end{array} \right) \Delta^2 f_0 + \left( \begin{array}{c} S \\ 3 \end{array} \right) \Delta^3 f_0 + ... + \left( \begin{array}{c} S \\ n \end{array} \right) \Delta^n f_0$
  - 其中，$S=\cfrac{x-x_0}{h}$，$h$ 為數據點間距。
    - 當 $S=0$ 時，$P_n(x_0)=f_0$；$S=1$ 時，$P_n(x_1)=f_0+(f_1-f_0)=f_1$；$S=2$ 時，$P_n(x_2)=f_2$；...

- 舉例
    > 假設已知：
    > | x | f(x) |
    > |:---:|:----:|
    > |0.4| 0.423 |
    > |0.6| 0.684 |
    > |0.8| 1.030 |
    > |1.0| 1.557 |
    >
    > 求 $f(0.73)$

  1. 因此可得差分表：
     - | $x_i$ | $f(x_i)$ | $\Delta f_i$ | $\Delta^2 f_i$ | $\Delta^3 f_i$ |
       |-------|:------:|------:|------|------|
       |  0.4  | <span style="color:red;">0.423</span> | <span style="color:red;">0.261</span> | <span style="color:red;">0,085</span> | <span style="color:red;">0.096</span> |
       |  0.6  |  0.684   | 0.346 | 0.181 | |
       |  0.8  |  1.030   | 0.527 |       | |
       |  1.0  |  1.557   |       |       | |
  2. 代入 $S=\cfrac{0.73-0.4}{0.2}=1.65$
  3. 故 $P_3(0.73)$ = $0.423$ + $1.65(0.261)$ + $\cfrac{1.65(1.65-1)}{2!}(0.085)$ + $\cfrac{1.65(1.65-1)(1.65-2)}{3!}(0.096)$ = $\underline{0.893\; \blacksquare}$

### 三次訪樣內插 (Cubic Spline Interpolation)
- 原理：每兩點之間以三次多項式內插，稱為三街訪樣。讓曲線在兩點之間更為平滑。
  - 儘管看起來此方法不如七階多項式內插精確，但是在實際應用中，三次訪樣內插更受歡迎。
  - 在已知點的任一個區間均為三次多項式的函數：$f_i(x)=a_ix^3+b_ix^2+c_ix+d_i$
    - 當已支點為 $n+1$ 個時（$i=0,1,2,...,n$），則有 $n$ 個區間（$n$ 個三次多項式）。與 $4n$ 個未知數。因此，須找透過以下五個條件找到 $4n$ 個未知數的 $4n$ 條件：
      1. 相鄰二條多項式在連接點的函數值相等（$2n-2$ 個方程式）。
      2. 第一個與最後一個函數必須通過端點（$2$ 個方程式）。
      3. 內部連接點的一階導數（斜率）相等（$n-1$ 個方程式）。
      4. 內部連接點的二階導數（曲率）相等（$n-1$ 個方程式）。
      5. 兩個端點的二階導數（曲率）為零（$2$ 個方程式）。
- 訪樣：
    - 線性訪樣：$f(x)=a_0 + a_1x$
    - 二階訪樣：斜率相等，$f(x)=a_0 + a_1x + a_2x^2$
    - 三階訪樣：斜率與曲率相等，$f(x)=a_0 + a_1x + a_2x^2 + a_3x^3$
- 步驟：
  1. 已知三階訪樣多項式：$g_i(x)$ = $a_i(x-x_i)^3$ + $b_i(x-x_i)^2$ + $c_i(x-x_i)$ + $d_i$，每個 $i$ 有四個未知數（$a_i, b_i, c_i, d_i$）。
  2. 通常設，$S_0=0$ 與 $S_n=0$ 兩個條件。
  3. $d_i$ = $y_i\quad i=0,1,2,...,n$
  4. $b_i$ = $\cfrac{S_i}{2}$
  5. $a_i$ = $\cfrac{S_{i+1}-S_i}{6h_i}\quad i=1,2,...,n-1$
  6. $c_i$ = $\cfrac{y_{i+1}-y_i}{h_i} - \cfrac{S_{i+1}+2S_i}{6}h_i\quad i=0,1,2,...,n-1$
  7. 求解聯立方程式。

- 舉例
    > At true function $f(x)=2e^x-x^2$  
    > 假設只知道以下數據：
    > 
    > | x | f(x) |
    > |---|------|
    > | 0.0 | 2.0000 |
    > | 1.0 | 4.4366 |
    > | 1.5 | 6.7134 |
    > | 2.25| 13.9130|
    >
    > 求 $f(0.66)、f(1.75)$

##### Solution
1. 設 $S_0=0$ 與 $S_3=0$
2. 計算一階除差：
   - | $x_i$ | $f(x_i)$ | $f[x_i,x_{i+1}]=f_i^{[1]}$ |
     |-------|:------:|------:|
     |  0.0  | 2.0000 | 2.4366 |
     |  1.0  | 4.4366 | 4.5536 |
     |  1.5  | 6.7134 | 9.5995 |
     |  2.25 | 13.9130|        |
3. 已知：$h_{i-1}S_{i-1}+2(h_{i-1}+h_i)S_i+h_iS_{i+1}=6\left ( \cfrac{y_{i+1}-y_i}{h_i} - \cfrac{y_i-y_{i-1}}{h_{i-1}} \right )\quad i=1,2,...,n-1$
4. 得：
   1. 當 $i=1$ 時：  
      $h_0S_0+2(h_0+h_1)S_1+h_1S_2$=$6\left ( \cfrac{6.7134-4.4366}{1.5} - \cfrac{4.4366-2.0000}{1.0} \right )$=$6(4.5536-2.4366)$=$13.7020$
   3. 當 $i=2$ 時：  
      $h_1S_1+2(h_1+h_2)S_2+h_2S_3$=$6\left ( \cfrac{13.9130-6.7134}{2.25} - \cfrac{6.7134-4.4366}{1.5} \right )$=$6(9.5995-4.5536)$=$30.2754$
5. 將 $S_1$ 與 $S_2$ 作為未知數求聯立方程式：
   1. $\begin{bmatrix} 2(h_0+h_1) & h_1 \\ h_1 & 2(h_1+h_2) \end{bmatrix} \begin{bmatrix} S_1 \\ S_2 \end{bmatrix} = \begin{bmatrix} 13.7020 \\ 30.2754 \end{bmatrix}$
   2. $\begin{bmatrix} 2(1.0+1.5) & 1.5 \\ 1.5 & 2(0.5+0.75) \end{bmatrix} \begin{bmatrix} S_1 \\ S_2 \end{bmatrix} = \begin{bmatrix} 13.7020 \\ 30.2754 \end{bmatrix}$
   3. 得：$S_1=2.2960$ 與 $S_2=11.6518$


## Carve Fitting (曲線擬合)
### 線性迴歸 (Linear Regression)
#### 最小平方近似法
- 最小平方近似法：
  - 原理：
    1. 直線的數學表示：$y=a_0+a_1x+e$
    2. 因此誤差：$e=y-a_0-a_1x$。需要找到誤差最小化的方法：
    3. 如果只對誤差總和求極小誤差，則 $\Sigma_{i=1}^n e_i=\Sigma_{i=1}^n(y_i-a_0-a_1x_i)$ 求最小值。但是誤差存在正負，因此可能會互相抵，使之可能產生無限多解，不符合目的。
    4. 如果改成使用誤差的絕對值，則 $\Sigma_{i=1}^n |e_i|=\Sigma_{i=1}^n|y_i-a_0-a_1x_i|$ 求最小值。可能出現兩個以上的解，因此不符合目的。
    5. 因此，透過誤差平方求最小誤差：$\Sigma_{i=1}^n e_i^2=\Sigma_{i=1}^n(y_i-a_0-a_1x_i)^2$ 求最小值。此時儘可能存在一組解，使得誤差平方和最小。
  - 算法：
    - $a_1=\cfrac{n\Sigma_{i=1}^n x_iy_i-\Sigma_{i=1}^n x_i\Sigma_{i=1}^n y_i}{n\Sigma_{i=1}^n x_i^2-(\Sigma_{i=1}^n x_i)^2}$
    - $a_0=\bar{y}-a_1\bar{x}$
  - 誤差量化：
    - **標準估計誤差 (Standard Error of The Estimate)** ($s_{y/x}$)：  
      $s_{y/x}=\sqrt{\cfrac{S_r}{n-2}}$  
      其中，殘餘數的平方和：$S_r=\Sigma_{i=1}^n(y_i-a_0-a_1x_i)^2$。
      - $s_{y/x}$ 表在迴歸直線附近的分散程度。
      - **總標準差** ($s_y$)：表平均值附近的分散程度：  
        $s_y=\sqrt{\cfrac{S_t}{n-1}}$。其中，總平方和：$S_t=\Sigma_{i=1}^n(y_i-\bar{y})^2$。
    - **決定係數 (Coefficient of Determination)** ($r^2$)：  
      $r^2=1-\cfrac{S_r}{S_t}$  
      $R^2$ 越接近 1，表示迴歸直線越好。其中，$(S_t-S_r)$ 表原始資料由平均值改由迴歸線描述的改善程度。
    - **相關係數 (Correlation Coefficient)** ($r$)：$r$
      - $r=\cfrac{n\Sigma_{i=1}^n x_iy_i-\Sigma_{i=1}^n x_i\Sigma_{i=1}^n y_i}{\sqrt{n\Sigma_{i=1}^n x_i^2-(\Sigma_{i=1}^n x_i)^2}\sqrt{n\Sigma_{i=1}^n y_i^2-(\Sigma_{i=1}^n y_i)^2}}$

### 非線性迴歸 (Nonlinear Regression)
- 非線性類別：
  - 指數函數：$y=a_1e^{b_1x}$→$\ln(y)=\ln(a_1)+b_1x$
  - 乘冪函數：$y=a_2x^{b_2}$→$\ln(y)=\ln(a_2)+b_2\ln(x)$
  - 飽和成長率方程式 (Saturating Growth Rate Equation)：$y=\cfrac{a_3x}{b_3+x}$→$\cfrac{1}{y}=\cfrac{b_3}{a_3}\cfrac{1}{x}+\cfrac{1}{a_3}$
- 方法：將其轉換為線性迴歸問題，並使用線性迴歸方法求解。最後，再將其轉換回原本的函數形式。

### 多項式迴歸 (Polynomial Regression)
- 原理：根據最小平方的原理，擴展為高階多項式 ($y=a_0+a_1x+a_2x^2+...+a_mx^m+e$)。
  - 因此，殘餘樹的平方總合 ($S_r$)：$S_r=\Sigma_{i=1}^n(y_i-a_0-a_1x_i-a_2x_i^2-...-a_mx_i^m)^2$
  - 標準誤差 ($s_{y/x}$)：$s_{y/x}=\sqrt{\cfrac{S_r}{n-(m+1)}}$
- 步驟：
  1. 找出 $n$、$\bar{x}$、$\bar{y}$、$\Sigma x_i$、$\Sigma y_i$、$\Sigma x_i^2$、$\Sigma x_i^3$、...、$\Sigma x_i^m$、$\Sigma x_iy_i$、$\Sigma x_i^2y_i$、...、$\Sigma x_i^my_i$。
  2. 根據：
    $$\begin{bmatrix} n & \Sigma x_i & \Sigma x_i^2 & ... & \Sigma x_i^m \\ \Sigma x_i & \Sigma x_i^2 & \Sigma x_i^3 & ... & \Sigma x_i^{m+1} \\ \Sigma x_i^2 & \Sigma x_i^3 & \Sigma x_i^4 & ... & \Sigma x_i^{m+2} \\ ... & ... & ... & ... & ... \\ \Sigma x_i^m & \Sigma x_i^{m+1} & \Sigma x_i^{m+2} & ... & \Sigma x_i^{2m} \end{bmatrix} \begin{bmatrix} a_0 \\ a_1 \\ a_2 \\ ... \\ a_m \end{bmatrix} = \begin{bmatrix} \Sigma y_i \\ \Sigma x_iy_i \\ \Sigma x_i^2y_i \\ ... \\ \Sigma x_i^my_i \end{bmatrix}$$
  3. 求解 $a_0$、$a_1$、$a_2$、...、$a_m$。

### 多重線性迴歸 (Multiple Linear Regression)
- 原理：根據多個變數的線性組合，進行迴歸分析。
  - $y=a_0+a_1x_1+a_2x_2+...+a_mx_m+e$
  - 殘差平方和：$S_r=\Sigma_{i=1}^n(y_i-a_0-a_1x_{i1}-a_2x_{i2}-...-a_mx_{im})^2$
  - 標準誤差：$s_{y/x}=\sqrt{\cfrac{S_r}{n-(m+1)}}$
  - 方程組：
    $$\begin{bmatrix} n & \Sigma x_{1i} & \Sigma x_{2i} & ... & \Sigma x_{mi} \\ \Sigma x_{1i} & \Sigma x_{1i}^2 & \Sigma x_{1i}x_{2i} & ... & \Sigma x_{1i}x_{mi} \\ \Sigma x_{2i} & \Sigma x_{1i}x_{2i} & \Sigma x_{2i}^2 & ... & \Sigma x_{2i}x_{mi} \\ ... & ... & ... & ... & ... \\ \Sigma x_{mi} & \Sigma x_{1i}x_{mi} & \Sigma x_{2i}x_{mi} & ... & \Sigma x_{mi}^2 \end{bmatrix} \begin{bmatrix} a_0 \\ a_1 \\ a_2 \\ ... \\ a_m \end{bmatrix} = \begin{bmatrix} \Sigma y_i \\ \Sigma x_{1i}y_i \\ \Sigma x_{2i}y_i \\ ... \\ \Sigma x_{mi}y_i \end{bmatrix}$$
- 多重乘冪迴歸 (Multiple Power Regression)
  - 原理：根據多個變數的乘冪組合，進行迴歸分析。
  - $y=a_0x_1^{_1}x_2^{b_2}...x_m^{b_m}$
  - 轉換為線性迴歸問題：$\ln(y)=\ln(a_0)+b_1\ln(x_1)+b_2\ln(x_2)+...+b_m\ln(x_m)$

---
<div style="display: grid; grid-template-columns: 1fr 4fr 1fr;">
  <div><a href="20241106_期中考題">< Last</a></div>
  <div></div>
  <div><a href="20241211_積分">Next ></a></div>
</div>